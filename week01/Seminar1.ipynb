{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to digital signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to https://github.com/markovka17/apdl/blob/master/week01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchaudio==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time domain $\\rightarrow$  frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load('c.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(wav: torch.Tensor, sr: int = 22050):\n",
    "    # Average all channels\n",
    "    if wav.dim() == 2:\n",
    "        # Any to mono audio convertion\n",
    "        wav = wav.mean(dim=0)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(wav, alpha=.7, c='green')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time', size=20)\n",
    "    plt.ylabel('Amplitude', size=20)\n",
    "    plt.show()\n",
    "    \n",
    "    display.display(display.Audio(wav, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_audio(wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "ft = torch.fft.fft(wav.mean(dim=0), n=n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude = ft.abs().pow(2)\n",
    "frequency = np.linspace(0, sr, len(magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency[:5000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spectrum\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(frequency, magnitude) # magnitude spectrum\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spectrum\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(frequency[:90], magnitude[:90]) # magnitude spectrum\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 523 * 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wave\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(wav.mean(0)[100:600])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load('example.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = torch.fft.rfft(wav, n=n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = torch.fft.rfft(wav.mean(dim=0), n=n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = spectrum.abs().pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(spectrogram.squeeze(), c='green')\n",
    "plt.grid()\n",
    "plt.xlabel('Frequency (Hz)', size=20)\n",
    "plt.ylabel('Magnitude$^2$', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = n_fft\n",
    "window = torch.hann_window(window_size)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(window, c='green')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_wav = wav[:, :window_size]\n",
    "windowed_clipped_wav = window * clipped_wav\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axes[0].plot(clipped_wav.squeeze(), c='green')\n",
    "axes[0].set_title('Raw Audio', size=20)\n",
    "\n",
    "axes[1].plot(windowed_clipped_wav.squeeze(), c='green')\n",
    "axes[1].set_title('Windowed Audio', size=20)\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].grid()\n",
    "    axes[i].set_xlabel('Time', size=20)\n",
    "    axes[i].set_ylabel('Amplitude', size=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torch.fft.rfft(clipped_wav).abs().pow(2)\n",
    "windowed_spectrogram = torch.fft.rfft(windowed_clipped_wav).abs().pow(2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axes[0].plot(spectrogram.squeeze(), c='green')\n",
    "axes[0].set_title('Spectrogram of Raw Audio', size=20)\n",
    "\n",
    "axes[1].plot(windowed_spectrogram.squeeze(), c='green')\n",
    "axes[1].set_title('Spectrogram of Windowed Audio', size=20)\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].grid()\n",
    "    axes[i].set_xlabel('Frequency (Hz)', size=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = torch.stft(\n",
    "    wav,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    window=torch.hann_window(1024),\n",
    "    \n",
    "    # We don't want to pad input signal\n",
    "    center=False,\n",
    "    \n",
    "    # Take first (n_fft // 2 + 1) frequencies\n",
    "    onesided=True,\n",
    "    \n",
    "    # Apply torch.view_as_real on each window\n",
    "    return_complex=False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = spectrum.norm(dim=-1).pow(2)\n",
    "spectrogram.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram.max(), spectrogram.min(), spectrogram.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(spectrogram.squeeze().data.numpy()[::-1,])\n",
    "plt.xlabel('Time', size=20)\n",
    "plt.ylabel('Frequency (Hz)', size=20)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(spectrogram.squeeze().log().data.numpy()[::-1,])\n",
    "plt.xlabel('Time', size=20)\n",
    "plt.ylabel('Frequency (Hz)', size=20)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_scaler = torchaudio.transforms.MelScale(\n",
    "    n_mels=80,\n",
    "    sample_rate=22_050,\n",
    "    n_stft=n_fft // 2 + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_scaler.fb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mel_scaler.fb.T)\n",
    "plt.xlabel('Hertz Scale', size=20)\n",
    "plt.ylabel('Mels Scale', size=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = mel_scaler(spectrogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mel_spectrogram.squeeze().log())\n",
    "plt.xlabel('Time', size=20)\n",
    "plt.ylabel('Mels', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio mnist classification\n",
    "\n",
    "![](https://i.imgur.com/OX1ADxu.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ouSOru91p-ZJCyI6E8cGh7N0r3vffi06' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ouSOru91p-ZJCyI6E8cGh7N0r3vffi06\" -O AudioMNIST.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to unzip archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q AudioMNIST.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим класс AudioMnistDataset для загрузки данных. При загрузке извлечем из названия файла label аудио - произносимую цифру - первый символ в аудио до знака \"_\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMnistDataset(Dataset):\n",
    "    SR = 16_000\n",
    "    \n",
    "    \"\"\"\n",
    "    Each wavfile has the following format: digit_speackerid_wavid.wav\n",
    "        For example, 6_01_47.wav:\n",
    "            6 -- the number 6 is spoken\n",
    "            01 -- the number is spoken by 1 speaker\n",
    "            47 -- id of wavfile        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path_to_data: str):\n",
    "        self.path_to_data = pathlib.Path(path_to_data)\n",
    "        self.paths = list(self.path_to_data.rglob('?_*_*.wav'))\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        path_to_wav = self.paths[index].as_posix()\n",
    "        \n",
    "        # Load wav\n",
    "        wav, sr = torchaudio.load(path_to_wav)\n",
    "        \n",
    "        label = int(path_to_wav.split('/')[-1].split('_')[0])\n",
    "        \n",
    "        return wav, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioMnistDataset('AudioMNIST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, label = dataset[123]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_audio(wav, sr=dataset.SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъем датасет на датасеты для трейна и валидации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "indexes = torch.randperm(len(dataset))\n",
    "train_indexes = indexes[:train_size]\n",
    "validation_indexes = indexes[train_size:]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indexes)\n",
    "validation_dataset = Subset(dataset, validation_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not set(train_indexes.tolist()).intersection(set(validation_indexes.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Collator объединяет аудиодорожки в один батч. Так как в нашей задаче все аудио разной длины, для того чтобы собрать в батч, заполним недостающее нулями (сделаем паддинг). Для этого создадим тензор из нулей размера [batch_size, max_wav_len] и заполним его элементами батча.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    \n",
    "    def __call__(self, batch: List[Tuple[torch.Tensor, int]]):\n",
    "        lengths = []\n",
    "        wavs, labels = zip(*batch)\n",
    "        \n",
    "        for wav in wavs:\n",
    "            lengths.append(wav.size(-1))\n",
    "        \n",
    "        max_len = max(lengths)\n",
    "        # your code\n",
    "\n",
    "        labels = torch.tensor(labels).long()\n",
    "        lengths = torch.tensor(lengths).long()\n",
    "        \n",
    "        return {\n",
    "            'wav': batch_wavs,\n",
    "            'label': labels,\n",
    "            'length': lengths,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=32,\n",
    "    shuffle=True, collate_fn=Collator(),\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=32,\n",
    "    collate_fn=Collator(),\n",
    "    num_workers=2, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Featurizer делает необходимый в нашей задаче препроцессинг - считаем мел спектрограмму, логарифмирует ее и считает длину спектрограммы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MELS = 80\n",
    "HOP_LEN = 256\n",
    "N_FFT = 1024\n",
    "WIN_LEN = N_FFT\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Featurizer, self).__init__()\n",
    "        \n",
    "        self.featurizer = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            win_length=WIN_LEN,\n",
    "            hop_length=HOP_LEN,\n",
    "            n_mels=NUM_MELS,\n",
    "        )\n",
    "        \n",
    "    def forward(self, wav, length=None):\n",
    "        mel_spectrogram = self.featurizer(wav)\n",
    "        mel_spectrogram = mel_spectrogram.clamp(min=1e-5).log()\n",
    "        \n",
    "        if length is not None:\n",
    "            length = (length - self.featurizer.win_length) // self.featurizer.hop_length\n",
    "            # We add `4` because in MelSpectrogram center==True\n",
    "            length += 4\n",
    "            \n",
    "            return mel_spectrogram, length\n",
    "        \n",
    "        return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модель приходит батч размера [batch_size, num_mels, seq_len].\n",
    "\n",
    "Последний слой - слой для классификации, возвращающий тензор размера [batch_size, NUM_CLASSES].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_channels):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "        # your code\n",
    "\n",
    "    def forward(self, inputs, length=None):\n",
    "        # inputs of shape [batch_size, num_mels, seq_len]\n",
    "        # your code\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры модели и оптимизатора и функцию потерь\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Model(input_dim=NUM_MELS, num_channels=32).to(device)\n",
    "featurizer = Featurizer().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс для подсчета метрики\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = defaultdict(list)\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    train_loss_meter = AverageMeter()\n",
    "    print(f\"Epoch {epoch} out of {num_epoch}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Move batch to device if device != 'cpu'\n",
    "        wav = batch['wav'].to(device)\n",
    "        length = batch['length'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # Compute mel spectrogram\n",
    "        mel, mel_length = featurizer(wav, length)\n",
    "\n",
    "        # feed model\n",
    "        output = model(mel, mel_length)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # zero out previously computed gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "    \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_meter.update(loss.item())\n",
    "        \n",
    "    storage['train_loss'].append(train_loss_meter.avg)\n",
    "    \n",
    "    validation_loss_meter = AverageMeter()\n",
    "    validation_accuracy_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in islice(enumerate(tqdm(validation_dataloader)), 1):\n",
    "        # Move batch to device if device != 'cpu'\n",
    "        wav = batch['wav'].to(device)\n",
    "        length = batch['length'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # in inference mode we don't need to compute gradients\n",
    "        # so we use `no_grad()` context manager to speed up inference\n",
    "        with torch.no_grad():\n",
    "\n",
    "            mel, mel_length = featurizer(wav, length)\n",
    "            output = model(mel, mel_length)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "        \n",
    "        # compute accuracy\n",
    "        matches = (output.argmax(dim=-1) == label).float().mean()\n",
    "\n",
    "        validation_loss_meter.update(loss.item())\n",
    "        validation_accuracy_meter.update(matches.item())\n",
    "    \n",
    "    storage['validation_loss'].append(validation_loss_meter.avg)\n",
    "    storage['validation_accuracy'].append(validation_accuracy_meter.avg)\n",
    "    \n",
    "    display.clear_output()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].plot(storage['train_loss'], label='train_loss')\n",
    "    axes[1].plot(storage['validation_loss'], label='validation_loss')\n",
    "\n",
    "    axes[2].plot(storage['validation_accuracy'], label='validation_accuracy')\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i].grid()\n",
    "        \n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataloader, take_n=10):\n",
    "    \"\"\"\n",
    "    Display wav and results of NN\n",
    "    \"\"\"\n",
    "    batch = next(iter(dataloader))\n",
    "    # your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "duration": 576.112316,
   "end_time": "2020-12-12T15:23:26.723203",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-12T15:13:50.610887",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
